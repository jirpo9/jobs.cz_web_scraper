import requests  # Pro odesÃ­lÃ¡nÃ­ HTTP poÅ¾adavkÅ¯ (stahovÃ¡nÃ­ obsahu webovÃ½ch strÃ¡nek).
from bs4 import BeautifulSoup  # Pro parsovÃ¡nÃ­ (analÃ½zu) HTML kÃ³du staÅ¾enÃ½ch webovÃ½ch strÃ¡nek.
import os  # Pro prÃ¡ci se soubory a sprÃ¡vu cest k souborÅ¯m.
import textwrap  # Pro zalamovÃ¡nÃ­ dlouhÃ½ch textÅ¯ na vÃ­ce Å™Ã¡dkÅ¯ pro lepÅ¡Ã­ Äitelnost.
import hashlib  # Pro hashovÃ¡nÃ­ dat (pro kontrolu jedineÄnosti) 
import logging          
# Knihovna pro logovÃ¡nÃ­ - zaznamenÃ¡vÃ¡nÃ­ prÅ¯bÄ›hu programu
import random          
# Knihovna pro generovÃ¡nÃ­ nÃ¡hodnÃ½ch ÄÃ­sel
from selenium import webdriver                  
# HlavnÃ­ nÃ¡stroj pro ovlÃ¡dÃ¡nÃ­ prohlÃ­Å¾eÄe
from selenium.webdriver.common.by import By    
# TÅ™Ã­da pro zpÅ¯soby vyhledÃ¡vÃ¡nÃ­ elementÅ¯ na strÃ¡nce
from selenium.webdriver.chrome.service import Service    
# TÅ™Ã­da pro sprÃ¡vu ChromeDriver sluÅ¾by
from selenium.webdriver.chrome.options import Options    
# TÅ™Ã­da pro nastavenÃ­ Chrome prohlÃ­Å¾eÄe
from selenium.webdriver.support.ui import WebDriverWait  
# TÅ™Ã­da pro ÄekÃ¡nÃ­ na elementy
from selenium.webdriver.support import expected_conditions as EC  
# Importuje podmÃ­nky pro ÄekÃ¡nÃ­ na elementy
from webdriver_manager.chrome import ChromeDriverManager  
# Importuje sprÃ¡vce ChromeDriveru pro automatickou aktualizaci
import time # Knihovna pro prÃ¡ci s Äasem a prodlevami
import concurrent.futures # Knihovna pro paralelnÃ­ zpracovÃ¡nÃ­
from selenium.common.exceptions import TimeoutException, NoSuchElementException # Importuje specifickÃ© vÃ½jimky Selenia
logging.basicConfig(# Nastavuje zÃ¡kladnÃ­ konfiguraci logovÃ¡nÃ­
    level=logging.INFO,  
    # Nastavuje ÃºroveÅˆ logovÃ¡nÃ­ na INFO (bude zobrazovat vÅ¡echny INFO zprÃ¡vy a vyÅ¡Å¡Ã­)
    format='%(asctime)s - %(levelname)s - %(message)s'  
    # Definuje formÃ¡t logÅ¯: Äas - ÃºroveÅˆ - zprÃ¡va
)
logger = logging.getLogger(__name__)  
# VytvÃ¡Å™Ã­ logger specifickÃ½ pro tento modul

def compute_job_hash(job_data):
    """ VytvoÅ™Ã­ unikÃ¡tnÃ­ hash pro inzerÃ¡t na zÃ¡kladÄ› nÃ¡zvu, firmy a popisu """
    job_string = f"{job_data['title']}{job_data['company']}{job_data.get('description', '')}"
    return hashlib.md5(job_string.encode('utf-8')).hexdigest()

def load_existing_hashes(file_path):
    """ NaÄte existujÃ­cÃ­ hashe ze souboru jobs.txt """
    hashes = set()
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as file:
            for line in file:
                if line.startswith("Hash:"):
                    hashes.add(line.strip().split(": ")[1])  # UloÅ¾Ã­ hash do mnoÅ¾iny
    return hashes


def setup_selenium_driver():    
# Definuje funkci pro nastavenÃ­ webovÃ©ho prohlÃ­Å¾eÄe
    chrome_options = Options()  
    # VytvÃ¡Å™Ã­ novÃ½ objekt pro nastavenÃ­ Chrome
    chrome_options.add_argument("--headless")  
    # Nastavuje Chrome pro bÄ›h bez grafickÃ©ho rozhranÃ­
    chrome_options.add_argument("--no-sandbox")  
    # VypÃ­nÃ¡ sandbox pro lepÅ¡Ã­ vÃ½kon
    chrome_options.add_argument("--disable-dev-shm-usage")  
    # Å˜eÅ¡Ã­ problÃ©my s pamÄ›tÃ­ v kontejnerech
    chrome_options.add_argument("--disable-extensions")  
    # VypÃ­nÃ¡ rozÅ¡Ã­Å™enÃ­ prohlÃ­Å¾eÄe
    chrome_options.add_argument("--disable-plugins")    
    # VypÃ­nÃ¡ pluginy prohlÃ­Å¾eÄe
    chrome_options.add_argument("--disable-images")     
    # VypÃ­nÃ¡ naÄÃ­tÃ¡nÃ­ obrÃ¡zkÅ¯
    chrome_options.page_load_strategy = 'eager'         
    # Nastavuje rychlejÅ¡Ã­ strategii naÄÃ­tÃ¡nÃ­ strÃ¡nek    
    service = Service(ChromeDriverManager().install())  
    # Inicializuje ChromeDriver s automatickou sprÃ¡vou verzÃ­
    driver = webdriver.Chrome(service=service, options=chrome_options)  
    # VytvÃ¡Å™Ã­ instanci ChromeDriveru s nastavenÃ½mi moÅ¾nostmi
    return driver  # VracÃ­ instanci ChromeDriveru

def human_like_delay():  # Definuje funkci pro simulaci lidskÃ©ho zpoÅ¾dÄ›nÃ­
    time.sleep(random.uniform(1, 2))  # NÃ¡hodnÄ› ÄekÃ¡ mezi 1 a 2 sekundami

def get_page_urls(base_url="https://www.jobs.cz/prace/python-vyvojar/"):  # Definuje funkci pro zÃ­skÃ¡nÃ­ URL strÃ¡nek
    urls = [base_url]  # VytvÃ¡Å™Ã­ seznam s vÃ½chozÃ­ URL
    for page in range(2, 11):  # Pro kaÅ¾dou strÃ¡nku od 2 do 10
        urls.append(f"{base_url}?profession%5B0%5D=201100585&page={page}")  
        # PÅ™idÃ¡vÃ¡ URL strÃ¡nky do seznamu
    return urls  # VracÃ­ seznam URL

def scrape_main_page():  # Definuje funkci pro scrapovÃ¡nÃ­ hlavnÃ­ strÃ¡nky
    driver = setup_selenium_driver()  # Nastavuje a inicializuje webdriver
    jobs_data = []  # VytvÃ¡Å™Ã­ prÃ¡zdnÃ½ seznam pro uklÃ¡dÃ¡nÃ­ dat o pracovnÃ­ch pozicÃ­ch
    seen_urls = set()  # VytvÃ¡Å™Ã­ prÃ¡zdnou mnoÅ¾inu pro sledovÃ¡nÃ­ jiÅ¾ navÅ¡tÃ­venÃ½ch URL
    seen_titles = set()  # VytvÃ¡Å™Ã­ prÃ¡zdnou mnoÅ¾inu pro sledovÃ¡nÃ­ jiÅ¾ zpracovanÃ½ch titulÅ¯
    page_urls = get_page_urls()  # ZÃ­skÃ¡vÃ¡ seznam URL strÃ¡nek
    previous_page_titles = set()  # VytvÃ¡Å™Ã­ prÃ¡zdnou mnoÅ¾inu pro sledovÃ¡nÃ­ titulÅ¯ z pÅ™edchozÃ­ strÃ¡nky
    
    try:
        for page_num, page_url in enumerate(page_urls, 1):  # Pro kaÅ¾dou URL strÃ¡nky
            logger.info(f"Starting scraping from page {page_num}: {page_url}")  # Loguje zaÄÃ¡tek scrapovÃ¡nÃ­ strÃ¡nky
            driver.get(page_url)  # NaÄÃ­tÃ¡ strÃ¡nku
            
            try:
                wait = WebDriverWait(driver, 10)  # VytvÃ¡Å™Ã­ instanci WebDriverWait s timeoutem 10 sekund
                result_container = wait.until(  # ÄŒekÃ¡ na pÅ™Ã­tomnost kontejneru s vÃ½sledky
                    EC.presence_of_element_located((By.ID, "search-result-container"))
                )
                
                human_like_delay()  # Simuluje lidskÃ© zpoÅ¾dÄ›nÃ­
                page_source = driver.page_source  # ZÃ­skÃ¡vÃ¡ zdrojovÃ½ kÃ³d strÃ¡nky
                soup = BeautifulSoup(page_source, "html.parser")  # Parsuje HTML pomocÃ­ BeautifulSoup
                
                page_duplicate_count = 0  # Inicializuje poÄet duplicit na strÃ¡nce
                new_jobs_on_page = 0  # Inicializuje poÄet novÃ½ch pracovnÃ­ch pozic na strÃ¡nce
                current_page_titles = set()  # VytvÃ¡Å™Ã­ prÃ¡zdnou mnoÅ¾inu pro sledovÃ¡nÃ­ titulÅ¯ na aktuÃ¡lnÃ­ strÃ¡nce
                
                job_cards = soup.select("#search-result-container > div.Stack.Stack--hasIntermediateDividers.Stack--hasStartDivider > article")  # Selektuje karty pracovnÃ­ch pozic
                
                if not job_cards:  # Pokud nejsou nalezeny Å¾Ã¡dnÃ© karty pracovnÃ­ch pozic
                    logger.info(f"No job cards found on page {page_num}, stopping pagination")  # Loguje informaci a ukonÄuje strÃ¡nkovÃ¡nÃ­
                    break
                
                logger.info(f"Found {len(job_cards)} job cards on page {page_num}")  # Loguje poÄet nalezenÃ½ch karet pracovnÃ­ch pozic
                
                for job in job_cards:  # Pro kaÅ¾dou kartu pracovnÃ­ch pozic
                    title_tag = job.select_one("header > h2")  # Selektuje tag s titulem
                    if title_tag:  # Pokud je tag s titulem nalezen
                        job_title = title_tag.get_text(strip=True)  # ZÃ­skÃ¡vÃ¡ text titulu
                        current_page_titles.add(job_title)  # PÅ™idÃ¡vÃ¡ titul do mnoÅ¾iny titulÅ¯ na aktuÃ¡lnÃ­ strÃ¡nce
                
                duplicate_with_previous = len(current_page_titles.intersection(previous_page_titles))  # PoÄÃ­tÃ¡ poÄet duplicitnÃ­ch titulÅ¯ s pÅ™edchozÃ­ strÃ¡nkou
                if duplicate_with_previous > len(current_page_titles) * 0.5:  # Pokud je vÃ­ce neÅ¾ polovina titulÅ¯ duplicitnÃ­
                    logger.info(f"Found {duplicate_with_previous} duplicate jobs from previous page, stopping pagination")  # Loguje informaci a ukonÄuje strÃ¡nkovÃ¡nÃ­
                    break
                
                for idx, job in enumerate(job_cards, 1):  # Pro kaÅ¾dou kartu pracovnÃ­ch pozic
                    try:
                        title_tag = job.select_one("header > h2")  # Selektuje tag s titulem
                        job_title = title_tag.get_text(strip=True) if title_tag else None  # ZÃ­skÃ¡vÃ¡ text titulu
                        
                        company_tag = job.select_one("footer > ul > li:nth-child(1) > span")  # Selektuje tag s nÃ¡zvem spoleÄnosti
                        company_name = company_tag.get_text(strip=True) if company_tag else None  # ZÃ­skÃ¡vÃ¡ text nÃ¡zvu spoleÄnosti
                        
                        link_tag = job.find("a", class_="SearchResultCard__titleLink", href=True)  # Selektuje tag s odkazem na detail pozice
                        detail_url = link_tag["href"] if link_tag else None  # ZÃ­skÃ¡vÃ¡ URL detailu pozice
                        if detail_url and not detail_url.startswith("http"):  # Pokud URL neobsahuje protokol
                            detail_url = "https://www.jobs.cz" + detail_url  # PÅ™idÃ¡vÃ¡ protokol k URL
                        
                        if job_title in seen_titles or detail_url in seen_urls:  # Pokud je titul nebo URL jiÅ¾ zpracovÃ¡na
                            page_duplicate_count += 1  # ZvyÅ¡uje poÄet duplicit na strÃ¡nce
                            continue  # PokraÄuje na dalÅ¡Ã­ kartu
                        
                        seen_titles.add(job_title)  # PÅ™idÃ¡vÃ¡ titul do mnoÅ¾iny zpracovanÃ½ch titulÅ¯
                        seen_urls.add(detail_url)  # PÅ™idÃ¡vÃ¡ URL do mnoÅ¾iny zpracovanÃ½ch URL
                        new_jobs_on_page += 1  # ZvyÅ¡uje poÄet novÃ½ch pracovnÃ­ch pozic na strÃ¡nce
                        
                        if job_title and company_name:  # Pokud je titul a nÃ¡zev spoleÄnosti nalezen
                            jobs_data.append({  # PÅ™idÃ¡vÃ¡ data o pracovnÃ­ pozici do seznamu
                                "title": job_title,
                                "company": company_name,
                                "url": detail_url
                            })
                            logger.info(f"Found new job {new_jobs_on_page} on page {page_num}: {job_title} at {company_name}")  # Loguje informaci o nalezenÃ© pracovnÃ­ pozici
                    
                    except Exception as e:  # Pokud nastane vÃ½jimka
                        logger.error(f"Error processing job card {idx} on page {page_num}: {str(e)}")  # Loguje chybu
                        continue  # PokraÄuje na dalÅ¡Ã­ kartu
                
                previous_page_titles = current_page_titles  # Aktualizuje mnoÅ¾inu titulÅ¯ z pÅ™edchozÃ­ strÃ¡nky
                
                if page_duplicate_count >= len(job_cards) * 0.5:  # Pokud je vÃ­ce neÅ¾ polovina karet duplicitnÃ­
                    logger.info(f"Found {page_duplicate_count} duplicate jobs out of {len(job_cards)} on page {page_num}, stopping pagination")  # Loguje informaci a ukonÄuje strÃ¡nkovÃ¡nÃ­
                    break
                
                if new_jobs_on_page == 0:  # Pokud nejsou nalezeny Å¾Ã¡dnÃ© novÃ© pracovnÃ­ pozice
                    logger.info("No new unique jobs found on this page, stopping pagination")  # Loguje informaci a ukonÄuje strÃ¡nkovÃ¡nÃ­
                    break
                
                human_like_delay()  # Simuluje lidskÃ© zpoÅ¾dÄ›nÃ­
                
            except Exception as e:  # Pokud nastane vÃ½jimka
                logger.error(f"Error processing page {page_num}: {str(e)}")  # Loguje chybu
                break  # UkonÄuje strÃ¡nkovÃ¡nÃ­
                
        logger.info(f"Successfully scraped total of {len(jobs_data)} unique jobs")  # Loguje ÃºspÄ›Å¡nÃ© scrapovÃ¡nÃ­
        return jobs_data  # VracÃ­ data o pracovnÃ­ch pozicÃ­ch
        
    except Exception as e:  # Pokud nastane vÃ½jimka
        logger.error(f"Error in main scraping process: {str(e)}")  # Loguje chybu
        return []  # VracÃ­ prÃ¡zdnÃ½ seznam
        
    finally:
        driver.quit()  # UkonÄuje webdriver

def scrape_job_detail(url, existing_data):
    logger.info(f"Starting to scrape job detail: {url}")
    
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            job_data = extract_job_details(soup, existing_data)
            if job_data:
                return job_data  # Pokud se podaÅ™Ã­ scrapnout pÅ™es BeautifulSoup, vrÃ¡tÃ­me data
    except requests.RequestException as e:
        logger.warning(f"Failed to scrape {url} with BeautifulSoup: {e}, switching to Selenium")
    
    # Pokud BeautifulSoup selÅ¾e, pouÅ¾ijeme Selenium
    driver = setup_selenium_driver()
    try:
        driver.get(url)
        wait = WebDriverWait(driver, 15)
        wait.until(lambda d: any([
            d.find_elements(By.CLASS_NAME, "typography-body-medium-text-regular"),
            d.find_elements(By.CLASS_NAME, "jobad__body"),
            d.find_elements(By.CLASS_NAME, "jobad__content")
        ]))
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, "html.parser")
        return extract_job_details(soup, existing_data)
    except TimeoutException:
        logger.error(f"Timeout while scraping {url} with Selenium")
        return existing_data
    finally:
        driver.quit()

def extract_job_details(soup, existing_data):
    job_data = existing_data.copy()
    
    job_body_tag = (soup.find("div", attrs={"data-test": "jd-body-richtext"}) or
                    soup.find("div", class_="jobad__body") or
                    soup.find("div", class_="jobad__content") or
                    soup.select_one(".jobad__html-content"))
    job_data["description"] = job_body_tag.get_text(strip=True).replace("PracovnÃ­ nabÃ­dka", "").strip() if job_body_tag else "Detail pozice nenalezen"
    
    contact_name_tag = (soup.find("a", attrs={"data-test": "jd-contact-company"}) or
                         soup.find("div", class_="jobad__contact-person") or
                         soup.select_one("[data-test='jd-contact-name']"))
    job_data["contact"] = contact_name_tag.get_text(strip=True) if contact_name_tag else "KontaktnÃ­ osoba nenalezena"
    
    contact_phone_tag = (soup.find("p", attrs={"data-test": "jd-contact-phone"}) or
                          soup.find("div", class_="jobad__contact-phone") or
                          soup.select_one("[data-test='jd-contact-phone']"))
    job_data["phone"] = contact_phone_tag.get_text(strip=True) if contact_phone_tag else "Telefon nenalezen"
    
    return job_data

counter = {"value": 0}
def save_job_to_file(job_data, file, existing_hashes):
    """ UloÅ¾Ã­ inzerÃ¡t do souboru pouze pokud jeÅ¡tÄ› neexistuje """
    job_hash = compute_job_hash(job_data)
    
    if job_hash in existing_hashes:
        counter["value"] += 1 
        logger.info(f"Skipping duplicate job: {job_data['title']} at {job_data['company']}")
        return  # NepÅ™idÃ¡ inzerÃ¡t, pokud uÅ¾ existuje

    # PÅ™idÃ¡me novÃ½ inzerÃ¡t do souboru
    template = """â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¼ {title}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¢ SpoleÄnost: {company}

ğŸ“ Lokalita: {location}

ğŸ“‹ Detail pozice:

  {description}

ğŸ‘¤ Kontakt: {contact}

ğŸ“ Telefon: {phone}

"""
    try:
        wrapped_description = textwrap.fill(
            job_data.get('description', 'Detail pozice nenalezen'),
            width=80, initial_indent="  ", subsequent_indent="  "
        )
        
        file.write(template.format(
            title=job_data.get('title', 'NÃ¡zev pozice nenalezen'),
            company=job_data.get('company', 'Firma nenalezena'),
            location=job_data.get('location', 'Lokalita nenalezena'),
            description=wrapped_description,
            contact=job_data.get('contact', 'KontaktnÃ­ osoba nenalezena'),
            phone=job_data.get('phone', 'Telefon nenalezen')
        ))
        logger.info(f"Saved new job: {job_data['title']} at {job_data['company']}")
    except Exception as e:
        logger.error(f"Error writing job data to file: {str(e)}")

def scrape_jobs():
    """ HlavnÃ­ funkce pro scrapovÃ¡nÃ­ pracovnÃ­ch pozic s hashovÃ¡nÃ­m """
    main_page_jobs = scrape_main_page()
    
    if not main_page_jobs:
        logger.error("No jobs found on main page")
        return

    successful_jobs = []
    file_path = os.path.join(os.getcwd(), "jobs.txt")
    existing_hashes = load_existing_hashes(file_path)  # NaÄte existujÃ­cÃ­ hashe

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        scrape_details = [executor.submit(scrape_job_detail, job['url'], job) for job in main_page_jobs]
        completed_jobs = concurrent.futures.as_completed(scrape_details)

        with open(file_path, "a", encoding="utf-8") as file:  # "a" = append mode
            for future in completed_jobs:
                detailed_job_data = future.result()
                if detailed_job_data:
                    save_job_to_file(detailed_job_data, file, existing_hashes)
                    successful_jobs.append(detailed_job_data)

    logger.info(f"Scraping completed. Successfully scraped {len(successful_jobs)- counter["value"]} jobs, {counter['value']} duplicates skipped.")
    logger.info(f"Data saved to '{file_path}'")

if __name__ == "__main__":  # Pokud je tento skript spuÅ¡tÄ›n jako hlavnÃ­
    scrape_jobs()  # SpouÅ¡tÃ­ hlavnÃ­ funkci pro scrapovÃ¡nÃ­ pracovnÃ­ch pozic
